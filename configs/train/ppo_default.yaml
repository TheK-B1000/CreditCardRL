# PPO hyperparameters (SB3)
algorithm: "PPO"

policy: "MlpPolicy"
total_timesteps: 500_000
learning_rate: 3.0e-4
n_steps: 2048
batch_size: 64
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5

# Network architecture
policy_kwargs:
  net_arch:
    pi: [256, 256]
    vf: [256, 256]

# Training settings
seed: 42
env_config: "configs/env/default_3card.yaml"
n_envs: 4                    # Parallel envs for vectorized training
eval_freq: 10_000            # Eval every N steps
eval_episodes: 50
checkpoint_freq: 50_000
log_dir: "runs/"
